{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford CS 329a Self-Improving AI Agents, Homework 1\n",
    "\n",
    "In this homework, we will go over basic techniques to scale test-time compute and self-improve. \n",
    "\n",
    "The homework will have 3 parts:\n",
    "\n",
    "1. Majority Voting\n",
    "2. Best-of-N with a Generative Reward Model\n",
    "3. Self-refinement\n",
    "4. [Bonus] Generative Process Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the relevant tools.\n",
    "## Make sure you installed this package with the instructions in README.md\n",
    "from cs329_hw1.tasks import MATH500\n",
    "from cs329_hw1.methods.verifiers import MATH500Verifier\n",
    "from cs329_hw1.methods import get_sampler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DEBUG_MODE = True # Set this to True when developing your code, set this to False when rendering your final numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset: MATH500\n",
    "We will work with a commonly used mathematical question answering dataset, MATH500.\n",
    "This is the subset of the larger MATH dataset, based on [Lightman et al. 2023](https://arxiv.org/abs/2305.20050).\n",
    "<br>\n",
    "First, let's get familiar with the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math500 = MATH500()\n",
    "problems = math500.get_problems(debug_mode=DEBUG_MODE)\n",
    "system_prompt = math500.get_system_prompt()\n",
    "\n",
    "# Each problem is a dictionary with a \"problem\" and an \"answer\"\n",
    "print(problems[0][\"problem\"])\n",
    "print(problems[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each problem is a dictionary with a \"problem\" and an \"answer\". \n",
    "<br>\n",
    "\"problem\" is the statement of the mathematical question, and \"answer\" is the correct answer to the question. Importantly, this is not a multiple-choice question, so the answer can often be a mathematical expression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Useful tools\n",
    "To help you implement the techniques we will cover in this homework, we implemented a few useful methods for you under `cs329_hw1.methods`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We implement a few useful methods for you under `methods`. \n",
    "\n",
    "# First, the simple multiple sampler.\n",
    "# This method returns a list of lists, where for each prompt that is passed as an input, we return a list of responses.\n",
    "# You will use this method as the basis of the techniques we will implement.\n",
    "# See the below example where we sample 3 responses from gpt-4o-mini with a temperature of 0.7.\n",
    "\n",
    "method = get_sampler(\"sample_multiple\", \"gpt-4o-mini\", temperature=0.7, system_prompt=system_prompt)\n",
    "prompts = [\"What language model are you based on?\"]\n",
    "responses = method(prompts)\n",
    "print(\"\\n\".join(responses[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on a single problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use this method function to get predictions for a given problem.\n",
    "# We get our first prediction from the method.\n",
    "# Let's get three predictions for the first problem, and read them out.\n",
    "problem_prompts = [problems[0][\"problem\"]]\n",
    "predictions = method(problem_prompts)[0]\n",
    "print(\"\\n\".join(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We give you a verifier tool for the MATH500 dataset.\n",
    "# The verifier for MATH500 is a simple tool that checks if the prediction is correct.\n",
    "# Internally, it parses the prediction and the answer and compares the final mathematical expression.\n",
    "# Importantly, the verifier is not perfect due to parsing challenges. \n",
    "# We mostly re-use the Qwen verifier based on the [Qwen-2.5 MATH repository](https://github.com/QwenLM/Qwen2.5-Math/tree/main/evaluation).\n",
    "# If you can be nerdsniped into writing a better verifier, we'd love to see it!\n",
    "\n",
    "# Let's test the predictions we got for the first problem that we have.\n",
    "verifier = MATH500Verifier()\n",
    "\n",
    "print(\"Correct answer: \", problems[0][\"answer\"])\n",
    "for prediction in predictions:\n",
    "    print(\"Last line of the prediction: \", prediction.split(\"\\n\\n\")[-1])\n",
    "    print(\"Is correct: \", verifier.verify(prediction, problems[0][\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's an example of how the verifier may actually fail:\n",
    "print(verifier.verify(solution=\"My final answer is \\frac{3\\pi}{2}\", ground_truth=\"3\\frac{\\pi}{2}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn!\n",
    "\n",
    "### 1- Evaluating zero-shot predictions (10 points)\n",
    "\n",
    "First, we will evaluate the accuracy of the predictions with a single sample, without using any test-time compute techniques. <br>\n",
    "You will only need to use the `method` we defined above and the `verifier` to compute the accuracy. This procedure makes 1 API call per problem.\n",
    "\n",
    "\n",
    "Deliverable: \n",
    "- Write your code in the section specified by `TODO: YOUR CODE STARTS HERE` and `TODO: YOUR CODE ENDS HERE`.\n",
    "- Report the accuracy of the predictions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = get_sampler(\"sample_multiple\", \"gpt-4o-mini\", temperature=0.7, n_samples=1, system_prompt=system_prompt)\n",
    "\n",
    "test_problems = problems\n",
    "\n",
    "### TODO: YOUR CODE STARTS HERE\n",
    "\n",
    "### TODO: YOUR CODE ENDS HERE\n",
    "### Report the accuracy of the predictions below.\n",
    "\n",
    "# print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Majority Voting (30 points)\n",
    "\n",
    "Here we will implement our first test-time compute technique, majority voting, as described in [this paper](https://arxiv.org/abs/2408.03314) or [this earlier paper](https://arxiv.org/abs/2203.11171). In particular,\n",
    "- You will sample multiple (in this case, 16) responses for each problem.\n",
    "- You will then take the majority vote as the prediction. The voting will be performed per each normalized expression (i.e., given the entire solution, we will parse the final numerical expression and perform the voting on that). We provide utility functions to do this.\n",
    "- You will then evaluate the prediction against the ground truth.\n",
    "\n",
    "Deliverable: \n",
    "- Write your code in the section specified by `TODO: YOUR CODE STARTS HERE` and `TODO: YOUR CODE ENDS HERE`.\n",
    "- Report the accuracy of the predictions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1 - Implementing majority voting.\n",
    "from typing import List, Union\n",
    "from cs329_hw1.methods.simple_samplers import SampleMultiple\n",
    "from cs329_hw1.tasks.math_utils import (\n",
    "    strip_string,\n",
    "    extract_answer,\n",
    ")\n",
    "\n",
    "class MajorityVoting:\n",
    "    \"\"\"\n",
    "    A class that implements majority voting strategy using multiple samples.\n",
    "    It generates multiple responses for each prompt and selects the most common answer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        system_prompt: str = None,\n",
    "        n_samples: int = 5,\n",
    "        temperature: float = 0.7,\n",
    "        max_workers: int = 256\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the majority voting method.\n",
    "\n",
    "        Args:\n",
    "            model (str): The name of the model to use\n",
    "            system_prompt (str, optional): System prompt to use for the model\n",
    "            n_samples (int, optional): Number of samples to generate per prompt. Defaults to 5.\n",
    "            temperature (float, optional): Temperature for sampling. Defaults to 0.7.\n",
    "        \"\"\"\n",
    "        self.sampler = SampleMultiple(\n",
    "            model=model,\n",
    "            system_prompt=system_prompt,\n",
    "            n_samples=n_samples,\n",
    "            temperature=temperature,\n",
    "            max_workers=max_workers\n",
    "        )\n",
    "\n",
    "    def _parse_answer(self, response: str) -> str:\n",
    "        return strip_string(extract_answer(response, \"math\"))\n",
    "\n",
    "    def _get_majority_answer(self, responses: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Determine the majority answer from a list of responses using a simple counter.\n",
    "\n",
    "        Args:\n",
    "            responses (List[str]): List of model responses\n",
    "\n",
    "        Returns:\n",
    "            str: The most common answer\n",
    "        \"\"\"\n",
    "        assert isinstance(responses, list), \"Responses must be a list\"\n",
    "        assert all(\n",
    "            isinstance(r, str) for r in responses\n",
    "        ), \"All responses must be strings\"\n",
    "        # Extract answers from responses\n",
    "\n",
    "        ## TODO: YOUR CODE STARTS HERE\n",
    "        ## Implement the majority voting logic here.\n",
    "        ## Feel free to use the `_parse_answer` method we implemented for you.\n",
    "        # Create a counter for each unique answer\n",
    "\n",
    "        ## TODO: YOUR CODE ENDS HERE\n",
    "        return majority_answer\n",
    "\n",
    "    def __call__(self, prompts: Union[str, List[str]], majority_voting_levels: List[int] = [1, 2, 4, 8, 16]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Execute majority voting on given prompt(s).\n",
    "\n",
    "        Args:\n",
    "            prompts (str or List[str]): The input prompt(s) to process\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: For each majority voting level, we return the majority answer.\n",
    "        \"\"\"\n",
    "        \n",
    "        ## TODO: YOUR CODE STARTS HERE\n",
    "        ## Implement the majority voting logic here.\n",
    "        ## Feel free to use the `_parse_answer` method we implemented for you.\n",
    "        ## len(majority_answers) should be equal to len(majority_voting_levels), and majority_answers[i] should be a list of strings of length len(prompts).\n",
    "        ## TODO: YOUR CODE ENDS HERE\n",
    "        return majority_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = MajorityVoting(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    n_samples=16,\n",
    "    temperature=0.7,\n",
    "    system_prompt=system_prompt,\n",
    "    max_workers=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not modify the code below; it is used to evaluate the accuracy of the predictions across different majority voting budgets.\n",
    "\n",
    "test_problems = problems\n",
    "\n",
    "total = len(test_problems)\n",
    "prompts = [problem[\"problem\"] for problem in test_problems]\n",
    "\n",
    "majority_voting_levels = [1, 2, 4, 8, 16]\n",
    "# Get all responses at once (this uses internal threading)\n",
    "all_responses_list = method(prompts)\n",
    "\n",
    "\n",
    "majority_voting_accuracies = []\n",
    "for responses_list, majority_voting_level in zip(all_responses_list, majority_voting_levels):\n",
    "    correct = 0\n",
    "    \n",
    "    for prediction, problem in zip(responses_list, test_problems):\n",
    "        # We do not re-normalize the prediction here, as we already normalized it in the majority voting step.\n",
    "        # To do so, we set the normalize_prediction flag to False.\n",
    "        if verifier.verify(prediction, problem[\"answer\"], normalize_prediction=False):\n",
    "            correct += 1\n",
    "        \n",
    "    accuracy = correct / len(test_problems)\n",
    "    majority_voting_accuracies.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter([1], [sum(zero_shot_correctness) / len(zero_shot_correctness)], s=100, color=\"darkorange\", marker=\"o\", label='No test-time compute')\n",
    "plt.plot(majority_voting_levels, majority_voting_accuracies, marker='o', color=\"green\", label='Majority Voting')\n",
    "plt.xlabel('Test-time Budget (# of API calls)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Number of Samples in Majority Voting')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Best-of-N with a Generative Reward Model (30 points)\n",
    "\n",
    "Here we will implement our second test-time compute technique, best-of-N with a generative reward model. In particular,\n",
    "- You will sample multiple (in this case, 16) responses for each problem.\n",
    "- You will then use an LLM to aggregate these responses and select the best answer. This is akin to best-of-N with a reward model as in [the paper](https://arxiv.org/abs/2408.03314), but we use an LLM as a judge instead of a discriminative reward model. This is sometimes referred to [Generative Verifiers](https://arxiv.org/abs/2408.15240) or [Generative Reward Models](https://arxiv.org/abs/2410.12832).\n",
    "- Overall, this procedure makes 16+1 API calls per problem.\n",
    "- You will then verify the prediction against the ground truth.\n",
    "- (Bonus, 2 points) We give a default prompt for the generative reward model. If you can improve the existing prompt and improve the accuracy of the technique compared to the numbers you get otherwise, you will get 2 points of extra credit. There is not an absolute number to beat, but ideally you observe +5% absolute percentage points improvement over the default prompt.\n",
    "\n",
    "Deliverables:\n",
    "- Write your code in the section specified by `TODO: YOUR CODE STARTS HERE` and `TODO: YOUR CODE ENDS HERE`.\n",
    "- Report the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMVoting:\n",
    "    \"\"\"\n",
    "    A class that uses an LLM to aggregate multiple responses and select the best answer.\n",
    "    It generates multiple responses for each prompt and uses another LLM call to choose the best answer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        system_prompt: str = None,\n",
    "        n_samples: int = 5,\n",
    "        temperature: float = 0.7,\n",
    "        max_workers: int = 256\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the LLM voting method.\n",
    "\n",
    "        Args:\n",
    "            model (str): The name of the model to use\n",
    "            system_prompt (str, optional): System prompt to use for the model\n",
    "            n_samples (int, optional): Number of samples to generate per prompt. Defaults to 5.\n",
    "            temperature (float, optional): Temperature for sampling. Defaults to 0.7.\n",
    "        \"\"\"\n",
    "        self.sampler = SampleMultiple(\n",
    "            model=model,\n",
    "            system_prompt=system_prompt,\n",
    "            n_samples=n_samples,\n",
    "            temperature=temperature,\n",
    "            max_workers=max_workers\n",
    "        )\n",
    "        \n",
    "        self.aggregator = SampleMultiple(\n",
    "            model=model,\n",
    "            system_prompt=system_prompt + \"\"\"\\n\\nYou are a mathematical expert. You will be shown multiple solutions to a math problem.\n",
    "Your task is to analyze these solutions and select the most likely correct answer.\"\"\",\n",
    "            n_samples=1,\n",
    "            temperature=0,  \n",
    "            max_workers=max_workers\n",
    "        )\n",
    "\n",
    "    def _create_aggregation_prompts(self, problems: List[str], all_responses: List[List[str]]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Create prompts for aggregation in parallel.\n",
    "        \n",
    "        Args:\n",
    "            problems (List[str]): List of original problems\n",
    "            all_responses (List[List[str]]): List of response lists for each problem\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of prompts for the aggregator\n",
    "        \"\"\"\n",
    "        return [\n",
    "            f\"\"\"Here is a math problem:\n",
    "{problem}\n",
    "\n",
    "I have received multiple solutions. Here they are:\n",
    "\n",
    "{chr(10).join(f'Solution {i+1}:{chr(10)}{r}' for i, r in enumerate(responses))}\n",
    "\n",
    "Based on these solutions, restate the solution here that you think is most likely correct.\"\"\"\n",
    "            for problem, responses in zip(problems, all_responses)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, prompts: Union[str, List[str]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Execute LLM-based voting on given prompt(s).\n",
    "\n",
    "        Args:\n",
    "            prompts (str or List[str]): The input prompt(s) to process\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: The chosen answer(s) and all responses\n",
    "        \"\"\"\n",
    "        # Get multiple samples for each prompt\n",
    "        ## TODO: YOUR CODE STARTS HERE\n",
    "        ## TODO: YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = LLMVoting(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    n_samples=16,\n",
    "    temperature=0.7,\n",
    "    system_prompt=system_prompt,\n",
    "    max_workers=128\n",
    ")\n",
    "\n",
    "test_problems = problems\n",
    "prompts = [problem[\"problem\"] for problem in test_problems]\n",
    "all_responses = method(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "correct_llm_voting = 0\n",
    "total_llm_voting = len(test_problems)\n",
    "for responses, problem in zip(all_responses, test_problems):\n",
    "    is_correct = verifier.verify(\n",
    "        responses[0],\n",
    "        problem[\"answer\"],\n",
    "        normalize_prediction=True\n",
    "    )\n",
    "    if is_correct:\n",
    "        correct_llm_voting += 1\n",
    "\n",
    "print(f\"\\nFinal accuracy: {correct_llm_voting/total_llm_voting:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(majority_voting_levels, majority_voting_accuracies, marker='o', color=\"green\", label='Majority Voting')\n",
    "# we sample 16 responses, and then we sample 1 response to aggregate them. thus, test-time budget is 16+1 API calls.\n",
    "plt.scatter([16+1], [correct_llm_voting/total_llm_voting], s=100, color=\"darkblue\", marker=\"s\", label='LLM Voting')\n",
    "plt.scatter([sum(zero_shot_correctness) / len(zero_shot_correctness)], [correct/total], s=100, color=\"darkorange\", marker=\"o\", label='No test-time compute')\n",
    "plt.xlabel('Test-time Budget (# of API calls)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Number of Samples')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a look at some of the aggregated responses, and the rationale there. \n",
    "## What patterns do you notice?\n",
    "print(all_responses[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Self-refinement\n",
    "\n",
    "Here we will implement our third test-time compute technique, self-refinement. This is akin to  In particular,\n",
    "- You will sample a single response for each problem.\n",
    "- You will then use an LLM to judge and refine this response. In the terminology of [this paper](https://arxiv.org/abs/2408.03314), in this case, both the refinement of the proposal distribution and the evaluation of a prediction are done by a generative model. However, using out-of-the-box LLMs for this step can be challenging to get right.\n",
    "- To match the test-time budget of 16 API calls, we will perform 16 iterations of refinement.\n",
    "- Finally, you will then verify the prediction against the ground truth.\n",
    "- (Bonus, 2 points) We give a default prompt for the refiner. If you can improve the existing refiner prompt and improve the accuracy of the technique by 5% absolute percentage points, you will get 2 points of extra credit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfRefinement:\n",
    "    \"\"\"\n",
    "    A class that implements iterative self-refinement strategy.\n",
    "    It generates an initial response and then repeatedly refines it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        system_prompt: str = None,\n",
    "        n_iterations: int = 3,\n",
    "        temperature: float = 0,\n",
    "        max_workers: int = 256\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the self-refinement method.\n",
    "\n",
    "        Args:\n",
    "            model (str): The name of the model to use\n",
    "            system_prompt (str, optional): System prompt to use for the model\n",
    "            n_iterations (int, optional): Number of refinement iterations. Defaults to 3.\n",
    "            temperature (float, optional): Temperature for sampling. Defaults to 0.\n",
    "        \"\"\"\n",
    "        # Initial solution generator\n",
    "        self.generator = SampleMultiple(\n",
    "            model=model,\n",
    "            system_prompt=system_prompt,\n",
    "            n_samples=1,\n",
    "            temperature=temperature,\n",
    "            max_workers=max_workers\n",
    "        )\n",
    "        \n",
    "        # Refinement sampler\n",
    "        self.refiner = SampleMultiple(\n",
    "            model=model,\n",
    "            system_prompt=\"\"\"You are a mathematical expert. You will be shown a math problem and a previous solution attempt.\n",
    "Your task is to carefully review the solution and provide an improved version.\n",
    "Focus on fixing any errors and making the solution more precise.\"\"\",\n",
    "            n_samples=1,\n",
    "            temperature=temperature,\n",
    "            max_workers=max_workers\n",
    "        )\n",
    "        \n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "    def _create_refinement_prompts(self, problems: List[str], current_solutions: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Create prompts for refinement in parallel.\n",
    "        \n",
    "        Args:\n",
    "            problems (List[str]): List of original problems\n",
    "            current_solutions (List[str]): List of current solutions to refine\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of prompts for the refiner\n",
    "        \"\"\"\n",
    "        return [\n",
    "            f\"\"\"Here is a math problem:\n",
    "{problem}\n",
    "\n",
    "Here is a previous solution attempt:\n",
    "{solution}\n",
    "\n",
    "Please provide an improved solution to this problem. Focus on accuracy and clarity.\"\"\"\n",
    "            for problem, solution in zip(problems, current_solutions)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, prompts: Union[str, List[str]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Execute self-refinement on given prompt(s).\n",
    "\n",
    "        Args:\n",
    "            prompts (str or List[str]): The input prompt(s) to process\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: The final refined answer(s) and intermediate solutions\n",
    "        \"\"\"\n",
    "        \n",
    "        ## TODO: YOUR CODE STARTS HERE\n",
    "        ## TODO: YOUR CODE ENDS HERE\n",
    "        # Return a list of lists where for each problem, we have the sequence of solutions.\n",
    "        # e.g., all_solutions[0] is the sequence of solutions for the first problem, and all_solutions[0][0] is the first solution for the first problem.\n",
    "\n",
    "method = SelfRefinement(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    n_iterations=16,\n",
    "    temperature=0.7,\n",
    "    system_prompt=system_prompt,\n",
    "    max_workers=1024\n",
    ")\n",
    "\n",
    "# Test it on your problems\n",
    "test_problems = problems\n",
    "prompts = [problem[\"problem\"] for problem in test_problems]\n",
    "final_responses = method(prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_accuracies = []\n",
    "n_iterations = len(final_responses[0])\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    correct = 0\n",
    "    for problem_iterations, problem in zip(final_responses, test_problems):\n",
    "        solution = problem_iterations[iteration]\n",
    "        is_correct = verifier.verify(\n",
    "            solution,\n",
    "            problem[\"answer\"],\n",
    "            normalize_prediction=True\n",
    "        )\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "    iteration_accuracies.append(correct/len(test_problems))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter([16+1], [correct_llm_voting/total_llm_voting], s=100, color=\"darkblue\", marker=\"s\", label='LLM Voting')\n",
    "plt.plot(majority_voting_levels, majority_voting_accuracies, marker='o', color=\"green\", label='Majority Voting')\n",
    "plt.scatter([sum(zero_shot_correctness) / len(zero_shot_correctness)], [correct/total], s=100, color=\"darkorange\", marker=\"o\", label='No test-time compute')\n",
    "iterations = list(range(n_iterations))\n",
    "plt.plot(iterations, iteration_accuracies, marker='^', color=\"red\", label='Self-Refinement')\n",
    "\n",
    "plt.xlabel('Test-time Budget (# of API calls)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Different Methods')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looking at the self-refinement results, what patterns do you notice?\n",
    "# let's look at a few examples\n",
    "print(final_responses[0][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Generative Process Reward Model (10 points)\n",
    "\n",
    "Here we will try to get some intuition about situations where process reward models can be useful compared to outcome reward models. In this exercise, we will try to generate a flawed solution for a problem, and demonstrate when a process reward model can be useful compared to an outcome reward model.\n",
    "\n",
    "- Your goal is to write a problem / solution pair.\n",
    "- The problem / solution pair should be such that the outcome reward model cannot find the mistake, but the process reward model can.\n",
    "- You can re-use the generative outcome / process reward model functions we wrote for you, or it is acceptable to write your own prompts / process reward models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: YOUR CODE STARTS HERE\n",
    "problem_text = r\"\"\" \"\"\"\n",
    "\n",
    "flawed_solution = r\"\"\" \"\"\"\n",
    "## TODO: YOUR CODE ENDS HERE\n",
    "\n",
    "def outcome_rm_judge_flaw(problem: str, solution: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Proposed Multi-Step Solution:\n",
    "{solution}\n",
    "\n",
    "Is this entire multi-step solution correct? Think step by step, then respond with \"Yes\" or \"No\".\n",
    "\"\"\"\n",
    "    sampler = SampleMultiple(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        system_prompt=None,\n",
    "        n_samples=1,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    response = sampler(prompt)[0][0]\n",
    "    return response\n",
    "\n",
    "\n",
    "def process_rm_judge_flaw(problem: str, solution: str) -> list:\n",
    "    \"\"\"\n",
    "    Asks the LLM to judge correctness of each step in parallel.\n",
    "    If a step is incorrect, the LLM should indicate the mistake.\n",
    "    We provide each step individually.\n",
    "    \"\"\"\n",
    "    lines = solution.strip().split(\"\\n\")\n",
    "    lines = [ln.strip() for ln in lines if ln.strip()]\n",
    "\n",
    "    # We'll treat each \"Step i)\" block as a separate unit to judge.\n",
    "    prompts = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Step \"):\n",
    "            prompt = f\"\"\"\n",
    "We will judge whether this step of the solution is correct or not.\n",
    "\n",
    "Problem:\n",
    "{problem}\n",
    "\n",
    "Step to judge:\n",
    "{line}\n",
    "\n",
    "If there's a hidden or subtle mistake (including numeric assumptions), please indicate it.\n",
    "Again, do NOT provide any corrections beyond identifying an error, if present.\n",
    "\"\"\"\n",
    "            prompts.append(prompt)\n",
    "\n",
    "    # Create a parallel sampler to check each step\n",
    "    sampler = SampleMultiple(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        system_prompt=None,\n",
    "        n_samples=1,\n",
    "        temperature=0.1,\n",
    "        max_workers=8\n",
    "    )\n",
    "\n",
    "    batch_responses = sampler(prompts)\n",
    "    return [resp_list[0] for resp_list in batch_responses]  # Flatten single responses\n",
    "\n",
    "# --------------------------------------------\n",
    "# Demonstration & Comparison of Both Methods\n",
    "# --------------------------------------------\n",
    "# 1) Outcome RM (entire solution at once)\n",
    "outcome_judgment = outcome_rm_judge_flaw(problem_text, flawed_solution)\n",
    "print(\"=== Outcome RM's Judgment (Single-Shot) ===\")\n",
    "print(outcome_judgment, \"\\n\")\n",
    "\n",
    "# 2) Process RM (step-by-step)\n",
    "process_judgments = process_rm_judge_flaw(problem_text, flawed_solution)\n",
    "print(\"=== Process RM's Judgment (Step-by-Step, Parallel) ===\")\n",
    "for idx, judge in enumerate(process_judgments, start=1):\n",
    "    print(f\"Step {idx} Judgment:\", judge, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
